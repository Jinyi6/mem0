{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48351a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始处理JSON文件 ---\n",
      "正在读取和处理: FirstAgentDataHighLevel_multiple_10.json...\n",
      "正在读取和处理: FirstAgentDataLowLevel_multiple_10.json...\n",
      "正在读取和处理: ThirdAgentDataHighLevel_multiple_10.json...\n",
      "正在读取和处理: ThirdAgentDataLowLevel_multiple_10.json...\n",
      "\n",
      "--- 所有文件处理完毕 ---\n",
      "已创建输出目录: processed_data\n",
      "\n",
      "✅ 合并后的文件已成功保存至: processed_data/merged_test_data.json\n",
      "\n",
      "--- 数据统计指标 ---\n",
      "每个分类下的题目数量:\n",
      "  - fh_Emotion_Emotion                     : 30 条\n",
      "  - fh_Preference_book                     : 30 条\n",
      "  - fh_Preference_food                     : 30 条\n",
      "  - fh_Preference_movie                    : 30 条\n",
      "  - fl_Aggregative_events                  : 23 条\n",
      "  - fl_Aggregative_roles                   : 23 条\n",
      "  - fl_Comparative_events                  : 23 条\n",
      "  - fl_Comparative_roles                   : 23 条\n",
      "  - fl_Knowledge_updating_events           : 23 条\n",
      "  - fl_Knowledge_updating_roles            : 23 条\n",
      "  - fl_Multi-hop_events                    : 23 条\n",
      "  - fl_Multi-hop_roles                     : 23 条\n",
      "  - fl_Multi-session-assistant_multi_agent : 23 条\n",
      "  - fl_Post_processing_events              : 23 条\n",
      "  - fl_Post_processing_roles               : 23 条\n",
      "  - fl_Single-hop_events                   : 23 条\n",
      "  - fl_Single-hop_roles                    : 23 条\n",
      "  - fl_Single-session-assistant_book       : 23 条\n",
      "  - fl_Single-session-assistant_food       : 23 条\n",
      "  - fl_Single-session-assistant_movie      : 23 条\n",
      "  - th_Emotion_Emotion                     : 15 条\n",
      "  - th_Preference_book                     : 15 条\n",
      "  - th_Preference_food                     : 15 条\n",
      "  - th_Preference_movie                    : 15 条\n",
      "  - tl_Aggregative_events                  : 12 条\n",
      "  - tl_Aggregative_hybrid                  : 12 条\n",
      "  - tl_Aggregative_roles                   : 12 条\n",
      "  - tl_Comparative_events                  : 12 条\n",
      "  - tl_Comparative_hybrid                  : 12 条\n",
      "  - tl_Comparative_roles                   : 12 条\n",
      "  - tl_Multi-hop_events                    : 12 条\n",
      "  - tl_Multi-hop_hybrid                    : 12 条\n",
      "  - tl_Multi-hop_items                     : 12 条\n",
      "  - tl_Multi-hop_places                    : 12 条\n",
      "  - tl_Multi-hop_roles                     : 12 条\n",
      "  - tl_Single-hop_events                   : 12 条\n",
      "  - tl_Single-hop_hybrid                   : 12 条\n",
      "  - tl_Single-hop_items                    : 12 条\n",
      "  - tl_Single-hop_places                   : 12 条\n",
      "  - tl_Single-hop_roles                    : 12 条\n",
      "  - tl_knowledge_updating_events           : 12 条\n",
      "  - tl_knowledge_updating_roles            : 12 条\n",
      "  - tl_post_processing_events              : 12 条\n",
      "  - tl_post_processing_hybrid              : 12 条\n",
      "  - tl_post_processing_items               : 12 条\n",
      "  - tl_post_processing_places              : 12 条\n",
      "  - tl_post_processing_roles               : 12 条\n",
      "------------------------------------------------\n",
      "  - 总计                                     : 824 条\n",
      "\n",
      "--- 任务完成 ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# --- 配置区 ---\n",
    "\n",
    "# 1. 定义输入文件与它们缩写的映射关系\n",
    "#    脚本会根据这个字典来寻找文件并生成新的字段\n",
    "FILE_MAPPING = {\n",
    "    \"FirstAgentDataHighLevel_multiple_10.json\": \"fh\",\n",
    "    \"FirstAgentDataLowLevel_multiple_10.json\": \"fl\",\n",
    "    \"ThirdAgentDataHighLevel_multiple_10.json\": \"th\",\n",
    "    \"ThirdAgentDataLowLevel_multiple_10.json\": \"tl\",\n",
    "}\n",
    "\n",
    "# 2. 定义输入文件所在的目录（\".\" 表示当前目录）\n",
    "INPUT_DIR = \"/Users/jinyi/Documents/code/memory/mem0/evaluation/dataset/Membench/data2test\" \n",
    "\n",
    "# 3. 定义合并后输出的文件名和目录\n",
    "OUTPUT_DIR = \"processed_data\"\n",
    "OUTPUT_FILENAME = \"merged_test_data.json\"\n",
    "\n",
    "# --- 脚本主逻辑 ---\n",
    "\n",
    "def merge_and_analyze_data():\n",
    "    \"\"\"\n",
    "    主函数，用于合并、处理JSON文件并进行统计分析。\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    category_counter = Counter()\n",
    "\n",
    "    print(\"--- 开始处理JSON文件 ---\")\n",
    "\n",
    "    # 遍历在配置中定义的每一个文件\n",
    "    for filename, short_name in FILE_MAPPING.items():\n",
    "        file_path = os.path.join(INPUT_DIR, filename)\n",
    "\n",
    "        # 检查文件是否存在\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ 警告：文件 '{filename}' 不存在，已跳过。\")\n",
    "            continue\n",
    "\n",
    "        print(f\"正在读取和处理: {filename}...\")\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # 根据你提供的格式，数据嵌套在两层动态的键下\n",
    "        # 例如：{\"Emotion\": {\"Emotion\": [...]}}\n",
    "        for top_category, inner_dict in data.items():\n",
    "            for inner_category, records_list in inner_dict.items():\n",
    "                \n",
    "                # 遍历核心记录列表\n",
    "                for record in records_list:\n",
    "                    # 获取tid，如果不存在则给一个默认值\n",
    "                    tid = record.get(\"tid\", \"UNKNOWN_TID\")\n",
    "\n",
    "                    # 1. 创建新的字段\n",
    "                    classification = f\"{short_name}_{top_category}_{inner_category}\"\n",
    "                    test_id = f\"{classification}_{tid}\"\n",
    "\n",
    "                    # 2. 为当前记录添加5个新字段\n",
    "                    record['test_id'] = test_id\n",
    "                    record['classification'] = classification\n",
    "                    record['file_type'] = short_name\n",
    "                    record['top_category'] = top_category\n",
    "                    record['inner_category'] = inner_category\n",
    "                    \n",
    "                    # 3. 将处理后的记录添加到总列表中\n",
    "                    merged_data.append(record)\n",
    "                    \n",
    "                    # 4. 更新分类计数器\n",
    "                    category_counter[classification] += 1\n",
    "    \n",
    "    print(\"\\n--- 所有文件处理完毕 ---\")\n",
    "\n",
    "    # 确保输出目录存在\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"已创建输出目录: {OUTPUT_DIR}\")\n",
    "\n",
    "    # 将合并后的数据写入新文件\n",
    "    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        # 使用 indent=4 格式化JSON，方便阅读\n",
    "        json.dump(merged_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n✅ 合并后的文件已成功保存至: {output_path}\")\n",
    "\n",
    "    # 打印统计指标\n",
    "    print(\"\\n--- 数据统计指标 ---\")\n",
    "    if not category_counter:\n",
    "        print(\"未处理任何数据。\")\n",
    "        return\n",
    "        \n",
    "    print(\"每个分类下的题目数量:\")\n",
    "    # 为了美观，找到最长的分类名以对齐输出\n",
    "    max_len = max(len(cat) for cat in category_counter.keys())\n",
    "    for category, count in sorted(category_counter.items()):\n",
    "        print(f\"  - {category:<{max_len}} : {count} 条\")\n",
    "        \n",
    "    total_questions = sum(category_counter.values())\n",
    "    print(\"-\" * (max_len + 10))\n",
    "    print(f\"  - {'总计':<{max_len}} : {total_questions} 条\")\n",
    "    print(\"\\n--- 任务完成 ---\")\n",
    "\n",
    "\n",
    "# 当脚本直接运行时，执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    merge_and_analyze_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28f6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个分类下的题目数量:\n",
      "  - Emotion     : 45 条\n",
      "  - book        : 68 条\n",
      "  - events      : 210 条\n",
      "  - food        : 68 条\n",
      "  - hybrid      : 60 条\n",
      "  - items       : 36 条\n",
      "  - movie       : 68 条\n",
      "  - multi_agent : 23 条\n",
      "  - places      : 36 条\n",
      "  - roles       : 210 条\n",
      "---------------------\n",
      "  - 总计          : 824 条\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "# 3. 定义合并后输出的文件名和目录\n",
    "OUTPUT_DIR = \"processed_data\"\n",
    "OUTPUT_FILENAME = \"merged_test_data.json\"\n",
    "\n",
    "# 读取合并后的数据文件\n",
    "output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    merged_data = json.load(f)\n",
    "\n",
    "# 按照 classification 统计各类别题目数量\n",
    "\n",
    "category_counter = Counter()\n",
    "for record in merged_data:\n",
    "    classification = record.get('inner_category', 'UNKNOWN')\n",
    "    category_counter[classification] += 1\n",
    "\n",
    "# 打印统计结果\n",
    "max_len = max(len(cat) for cat in category_counter.keys())\n",
    "print(\"每个分类下的题目数量:\")\n",
    "for category, count in sorted(category_counter.items()):\n",
    "    print(f\"  - {category:<{max_len}} : {count} 条\")\n",
    "print(\"-\" * (max_len + 10))\n",
    "print(f\"  - {'总计':<{max_len}} : {sum(category_counter.values())} 条\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
